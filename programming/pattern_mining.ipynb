{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Market Basket Analysis Dataset\n",
    "\n",
    "In order to be able to make a well-founded judgement on the quality of the final association rules that will result from the application of the techniques seen in the Market Basket Analysis course, I have created a set of superficial transactions whose underlying dynamics I know by construction.\n",
    "\n",
    "**Context:**\n",
    "\n",
    "We are located in a bookshop that offers nine different types of books. Each entry in the **transactions** dataset records a transaction made by a customer of this bookshop.\n",
    "\n",
    "Each customer behaves as follows:\n",
    "\n",
    "1. He randomly goes to a book genre **M** in the bookshop as soon as he arrives. (He doesn't necessarily buy a book of genre M.)\n",
    "3. The probability that he will buy a book genre **N** is inversely proportional to the distance between **M** and **N**, as defined by the order in **bookstore_genres**.\n",
    "2. He buys a minimum of two books and a maximum of **max_nb_of_elements_in_transactions** books.\n",
    "3. Each transaction has at most one genre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['history', 'cooking', 'health', 'biography', 'humor'], dtype='<U9')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bookstore_genres = [\"fiction\",\"poetry\",\"history\",\"biography\",\"cooking\",\"health\",\"travel\",\"language\",\"humor\"]\n",
    "number_of_bookstore_genres = len(bookstore_genres)\n",
    "number_of_transactions = 10000\n",
    "\n",
    "probability_distributions = np.ones((number_of_bookstore_genres,number_of_bookstore_genres))\n",
    "\n",
    "for i in range(number_of_bookstore_genres):\n",
    "    for j in range(1,number_of_bookstore_genres):\n",
    "        if i-j >= 0:\n",
    "            probability_distributions[i][i-j] /= 2**j\n",
    "        if i+j < number_of_bookstore_genres:\n",
    "            probability_distributions[i][i+j] /= 2**j\n",
    "\n",
    "probability_distributions /= probability_distributions.sum(axis=1)[:,None]\n",
    "\n",
    "\n",
    "transactions = []\n",
    "max_nb_of_elements_in_transactions = 5\n",
    "\n",
    "for _ in range(number_of_transactions):\n",
    "    first_item_idx_in_transaction = np.random.choice(number_of_bookstore_genres)\n",
    "    nb_of_items_in_transaction = np.random.randint(2, max_nb_of_elements_in_transactions+1)\n",
    "\n",
    "    transaction = np.random.choice(bookstore_genres,(nb_of_items_in_transaction),False, p=probability_distributions[first_item_idx_in_transaction])\n",
    "\n",
    "    transactions.append(transaction)\n",
    "\n",
    "\n",
    "transactions[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Market Basket Analysis\n",
    "\n",
    "**Let's define some context :**\n",
    "\n",
    "* Let $D^{m \\times n}$ be a onehot encoded dataset with each column representing a unique object $O_{j}$ and each row representing an event $E_{i}$ s.t. :\n",
    "\n",
    "    **$D_{ij}$** is True if the object $O_{j}$ appears in the event $E_{i}$, and False otherwise.\n",
    "\n",
    "* Let $A$ be the antecedent and $B$ the consequent.  \n",
    "\n",
    "* $A$ or $B$ are defined as a set of objects (called itemset) {$O_{j1}$, $O_{j2}$, ..., $O_{jk}$}, s.t. $O_{ji}$ represents some object in $D$ and $k >= 1$ is a natural number. \n",
    "\n",
    "* $A$ and $B$ are disjoint itemsets.\n",
    "\n",
    "* $A \\to B$ is an association rule. (the probabilistic equivalent of the \"if-then\" relation in logic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1\n",
    "\n",
    "* **Support** : \n",
    "    + **$Support(A \\to B)$** : This is the number of events in which the union of the set of objects in $A$ and the set of objects in $B$ (i.e. $A \\cup B$ ) appears divided by the total number of events in $D$.\n",
    "    + If we set $B$ to be $A$, (i.e. **$Support(A \\to A)$** ), this is the number of events in which the set of objects in $A$ appear divided by the total number of events in $D$.\n",
    "    + Support($A \\to A$) is also written as Support($A$).\n",
    "    + Suppport is a symmetric metric : $Support(A \\to B) = Support(B \\to A)$\n",
    "* **Confidence** :\n",
    "    + **$Confidence(A \\to B) = \\frac{Support(A \\to B)}{Support(A)}$**\n",
    "    + Confidence might be interpreted as follows : If $A$ appears in an event $E_{i}$ then what is the probability that $B$ appears in $E_{i}$.\n",
    "* **Lift** :\n",
    "    + **$Lift(A \\to B) = \\frac{Confidence(A \\to B)}{Support(B)} = \\frac{Support(A \\to B)}{Support(B)Support(A)}$**\n",
    "    + Lift is a metric that compare the observed frequency of the union $A \\cup B$ in $D$ and the frequency of the union $A \\cup B$ in $D$ if $A$ and $B$ are independent.\n",
    "    + If $Lift = 1$, then $A$ and $B$ are independent, i.e. if $A$ appears in an event $E_{i}$, then it doesn't impact the probability of $B$ appearing in $E_{i}$ which stays the same : Support($B$).\n",
    "    + If $Lift > 1$, then if $A$ appears in an event $E_{i}$, then it impact positively the probability of $B$ appearing in $E_{i}$. i.e. $P(B \\in E_{i} ⎹ A \\in E_{i}) > Support(B)$.\n",
    "    + If $Lift < 1$, then if $A$ appears in an event $E_{i}$, then it impact negatively the probability of $B$ appearing in $E_{i}$. i.e. $P(B \\in E_{i} ⎹ A \\in E_{i}) < Support(B)$.\n",
    "    + $Lift$ is a symmetric metric : $Lift(A \\to B) = Lift(B \\to A)$\n",
    "\n",
    "* **Leverage** : \n",
    "    + **$Leverage(A \\to B) = Support(A \\to B) - Support(B)Support(A)$**\n",
    "    + $Leverage$ is a very similar metric to $Lift$, but it uses a difference rather than a division, which changes the range and the interpretation of the outputs.\n",
    "    + If $Leverage = 0$, then $A$ and $B$ are independent.\n",
    "    + If $Leverage$ tends towards 1, $P(B \\in E_{i} ⎹ A \\in E_{i}) > Support(B)$.\n",
    "    + If $Leverage$ tends towards -1, $P(B \\in E_{i} ⎹ A \\in E_{i}) < Support(B)$.\n",
    "    + $Leverage$ is a symmetric metric : $Leverage(A \\to B) = Leverage(B \\to A)$\n",
    "* **Conviction** :\n",
    "    + **$Conviction(A \\to B) = \\frac{1 - Support(B)}{1 - Confidence(A \\to B)} = \\frac{Support(A) - Support(A)Support(B)}{Support(A) - Support(A \\to B)}$**\n",
    "    + If $Conviction = 1$, then $A$ and $B$ are independent.\n",
    "    + The more the presence of A positively impacts the presence of B in an event, the more $Conviction$ tends towards infinity.\n",
    "    + The more the presence of A negatively impacts the presence of B in an event, the more $Conviction$ tends towards 0.\n",
    "    + $Conviction$ is not symmetric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2\n",
    "\n",
    "#### 2.1\n",
    "\n",
    "To better understand the usefulness of the Pruning concept, we need to start by understanding the general approach to Market Basket Analysis:\n",
    "\n",
    "1. Given that there are $n$ objects in $D^{m \\times n}$, generate the set of all itemsets : $S = \\{s ⎹ s = \\{O_{j1}, O_{j2}, ..., O_{jk}\\}, \\forall k \\in [1,...,n]\\}$\n",
    "\n",
    "2. $\\forall s \\in S$, generate all possible pairs $P = \\{(A,B) ⎹ $A \\cup B = s$ and $A \\cap B =\\{\\}\\}$. \n",
    "(Reminder : $A$ is the antecedent and $B$ the consequent)\n",
    "\n",
    "3. Given a pair $p \\in P$, we can now compute different metrics on it and consider to keep it or not depending on a previously defined threshold.\n",
    "\n",
    "Now, the notion of Pruning intervene because we generally can't compute the complete set $S$ because the size of $S$ is $2^n - 1$. \n",
    "In concrete terms, we apply the notion of Pruning if we use any technique that consists of calculating only a subset of $S$ : For example, we could consider calculating the subset of $S$ only for $k = 3$.\n",
    "\n",
    "Another Pruning technique is the **Apriori algorithm** which rely on two main principles :\n",
    "* Subsets of frequent sets are frequent.\n",
    "* If $\\{O_{j1}, O_{j2}, ..., O_{jk}\\}$ is infrequent, so is $\\{O_{j1}, O_{j2}, ..., O_{jk}, O_{j(k+1)}\\}$.\n",
    "\n",
    "Consequently :\n",
    "\n",
    "1. We can avoid calculating all the sets of $S$ containing $O_{j}$ if $\\{O_{j}\\}$ is not frequent in $D$.\n",
    "2. We can avoid calculating all the sets of $S$ containing $O_{j1},O_{j2}$ if $\\{O_{j1},O_{j2}\\}$ is not frequent in $D$. Even if, both $O_{j1}$ and $O_{j2}$ are independently frequent in $D$.\n",
    "3. And so on..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 \n",
    "\n",
    "**min_support = 0.1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>support</th>\n",
       "      <th>itemsets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.4424</td>\n",
       "      <td>(biography)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.4698</td>\n",
       "      <td>(cooking)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.2892</td>\n",
       "      <td>(fiction)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.4575</td>\n",
       "      <td>(health)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.4233</td>\n",
       "      <td>(history)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.2862</td>\n",
       "      <td>(humor)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.3632</td>\n",
       "      <td>(language)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.3584</td>\n",
       "      <td>(poetry)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.4196</td>\n",
       "      <td>(travel)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.2226</td>\n",
       "      <td>(biography, cooking)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.1496</td>\n",
       "      <td>(biography, fiction)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.1865</td>\n",
       "      <td>(health, biography)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.2332</td>\n",
       "      <td>(history, biography)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.1953</td>\n",
       "      <td>(biography, poetry)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.1383</td>\n",
       "      <td>(travel, biography)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.1133</td>\n",
       "      <td>(fiction, cooking)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.2313</td>\n",
       "      <td>(health, cooking)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.1973</td>\n",
       "      <td>(history, cooking)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.1152</td>\n",
       "      <td>(humor, cooking)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.1527</td>\n",
       "      <td>(language, cooking)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.1511</td>\n",
       "      <td>(poetry, cooking)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.1955</td>\n",
       "      <td>(travel, cooking)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.1940</td>\n",
       "      <td>(history, fiction)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.2034</td>\n",
       "      <td>(fiction, poetry)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.1480</td>\n",
       "      <td>(health, history)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.1555</td>\n",
       "      <td>(health, humor)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.2015</td>\n",
       "      <td>(health, language)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.1035</td>\n",
       "      <td>(health, poetry)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.2416</td>\n",
       "      <td>(travel, health)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.2338</td>\n",
       "      <td>(history, poetry)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.2030</td>\n",
       "      <td>(humor, language)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.1905</td>\n",
       "      <td>(travel, humor)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.2337</td>\n",
       "      <td>(travel, language)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.1160</td>\n",
       "      <td>(history, biography, cooking)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.1083</td>\n",
       "      <td>(history, biography, fiction)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.1085</td>\n",
       "      <td>(biography, fiction, poetry)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.1375</td>\n",
       "      <td>(history, biography, poetry)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.1225</td>\n",
       "      <td>(travel, health, cooking)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.1476</td>\n",
       "      <td>(history, fiction, poetry)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.1110</td>\n",
       "      <td>(health, humor, language)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.1126</td>\n",
       "      <td>(travel, humor, health)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.1407</td>\n",
       "      <td>(travel, health, language)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.1457</td>\n",
       "      <td>(travel, humor, language)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    support                       itemsets\n",
       "0    0.4424                    (biography)\n",
       "1    0.4698                      (cooking)\n",
       "2    0.2892                      (fiction)\n",
       "3    0.4575                       (health)\n",
       "4    0.4233                      (history)\n",
       "5    0.2862                        (humor)\n",
       "6    0.3632                     (language)\n",
       "7    0.3584                       (poetry)\n",
       "8    0.4196                       (travel)\n",
       "9    0.2226           (biography, cooking)\n",
       "10   0.1496           (biography, fiction)\n",
       "11   0.1865            (health, biography)\n",
       "12   0.2332           (history, biography)\n",
       "13   0.1953            (biography, poetry)\n",
       "14   0.1383            (travel, biography)\n",
       "15   0.1133             (fiction, cooking)\n",
       "16   0.2313              (health, cooking)\n",
       "17   0.1973             (history, cooking)\n",
       "18   0.1152               (humor, cooking)\n",
       "19   0.1527            (language, cooking)\n",
       "20   0.1511              (poetry, cooking)\n",
       "21   0.1955              (travel, cooking)\n",
       "22   0.1940             (history, fiction)\n",
       "23   0.2034              (fiction, poetry)\n",
       "24   0.1480              (health, history)\n",
       "25   0.1555                (health, humor)\n",
       "26   0.2015             (health, language)\n",
       "27   0.1035               (health, poetry)\n",
       "28   0.2416               (travel, health)\n",
       "29   0.2338              (history, poetry)\n",
       "30   0.2030              (humor, language)\n",
       "31   0.1905                (travel, humor)\n",
       "32   0.2337             (travel, language)\n",
       "33   0.1160  (history, biography, cooking)\n",
       "34   0.1083  (history, biography, fiction)\n",
       "35   0.1085   (biography, fiction, poetry)\n",
       "36   0.1375   (history, biography, poetry)\n",
       "37   0.1225      (travel, health, cooking)\n",
       "38   0.1476     (history, fiction, poetry)\n",
       "39   0.1110      (health, humor, language)\n",
       "40   0.1126        (travel, humor, health)\n",
       "41   0.1407     (travel, health, language)\n",
       "42   0.1457      (travel, humor, language)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onehot_encoder = TransactionEncoder()\n",
    "transactions_onehot_encoding = onehot_encoder.fit(transactions).transform(transactions)\n",
    "df_transactions_onehot_encoding = pd.DataFrame(transactions_onehot_encoding, columns=onehot_encoder.columns_)\n",
    "\n",
    "frequent_itemsets = apriori(df_transactions_onehot_encoding,min_support=0.1,use_colnames=True)\n",
    "\n",
    "frequent_itemsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>antecedents</th>\n",
       "      <th>consequents</th>\n",
       "      <th>antecedent support</th>\n",
       "      <th>consequent support</th>\n",
       "      <th>support</th>\n",
       "      <th>confidence</th>\n",
       "      <th>lift</th>\n",
       "      <th>leverage</th>\n",
       "      <th>conviction</th>\n",
       "      <th>zhangs_metric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(biography)</td>\n",
       "      <td>(cooking)</td>\n",
       "      <td>0.4424</td>\n",
       "      <td>0.4698</td>\n",
       "      <td>0.2226</td>\n",
       "      <td>0.503165</td>\n",
       "      <td>1.071019</td>\n",
       "      <td>0.014760</td>\n",
       "      <td>1.067154</td>\n",
       "      <td>0.118919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(cooking)</td>\n",
       "      <td>(biography)</td>\n",
       "      <td>0.4698</td>\n",
       "      <td>0.4424</td>\n",
       "      <td>0.2226</td>\n",
       "      <td>0.473819</td>\n",
       "      <td>1.071019</td>\n",
       "      <td>0.014760</td>\n",
       "      <td>1.059711</td>\n",
       "      <td>0.125065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(biography)</td>\n",
       "      <td>(fiction)</td>\n",
       "      <td>0.4424</td>\n",
       "      <td>0.2892</td>\n",
       "      <td>0.1496</td>\n",
       "      <td>0.338156</td>\n",
       "      <td>1.169279</td>\n",
       "      <td>0.021658</td>\n",
       "      <td>1.073968</td>\n",
       "      <td>0.259634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(fiction)</td>\n",
       "      <td>(biography)</td>\n",
       "      <td>0.2892</td>\n",
       "      <td>0.4424</td>\n",
       "      <td>0.1496</td>\n",
       "      <td>0.517289</td>\n",
       "      <td>1.169279</td>\n",
       "      <td>0.021658</td>\n",
       "      <td>1.155143</td>\n",
       "      <td>0.203675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(health)</td>\n",
       "      <td>(biography)</td>\n",
       "      <td>0.4575</td>\n",
       "      <td>0.4424</td>\n",
       "      <td>0.1865</td>\n",
       "      <td>0.407650</td>\n",
       "      <td>0.921452</td>\n",
       "      <td>-0.015898</td>\n",
       "      <td>0.941336</td>\n",
       "      <td>-0.135794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>(travel, language)</td>\n",
       "      <td>(humor)</td>\n",
       "      <td>0.2337</td>\n",
       "      <td>0.2862</td>\n",
       "      <td>0.1457</td>\n",
       "      <td>0.623449</td>\n",
       "      <td>2.178368</td>\n",
       "      <td>0.078815</td>\n",
       "      <td>1.895626</td>\n",
       "      <td>0.705912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>(humor, language)</td>\n",
       "      <td>(travel)</td>\n",
       "      <td>0.2030</td>\n",
       "      <td>0.4196</td>\n",
       "      <td>0.1457</td>\n",
       "      <td>0.717734</td>\n",
       "      <td>1.710520</td>\n",
       "      <td>0.060521</td>\n",
       "      <td>2.056216</td>\n",
       "      <td>0.521182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>(travel)</td>\n",
       "      <td>(humor, language)</td>\n",
       "      <td>0.4196</td>\n",
       "      <td>0.2030</td>\n",
       "      <td>0.1457</td>\n",
       "      <td>0.347235</td>\n",
       "      <td>1.710520</td>\n",
       "      <td>0.060521</td>\n",
       "      <td>1.220961</td>\n",
       "      <td>0.715683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>(humor)</td>\n",
       "      <td>(travel, language)</td>\n",
       "      <td>0.2862</td>\n",
       "      <td>0.2337</td>\n",
       "      <td>0.1457</td>\n",
       "      <td>0.509085</td>\n",
       "      <td>2.178368</td>\n",
       "      <td>0.078815</td>\n",
       "      <td>1.560961</td>\n",
       "      <td>0.757832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>(language)</td>\n",
       "      <td>(travel, humor)</td>\n",
       "      <td>0.3632</td>\n",
       "      <td>0.1905</td>\n",
       "      <td>0.1457</td>\n",
       "      <td>0.401156</td>\n",
       "      <td>2.105808</td>\n",
       "      <td>0.076510</td>\n",
       "      <td>1.351772</td>\n",
       "      <td>0.824628</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>108 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            antecedents         consequents  antecedent support  \\\n",
       "0           (biography)           (cooking)              0.4424   \n",
       "1             (cooking)         (biography)              0.4698   \n",
       "2           (biography)           (fiction)              0.4424   \n",
       "3             (fiction)         (biography)              0.2892   \n",
       "4              (health)         (biography)              0.4575   \n",
       "..                  ...                 ...                 ...   \n",
       "103  (travel, language)             (humor)              0.2337   \n",
       "104   (humor, language)            (travel)              0.2030   \n",
       "105            (travel)   (humor, language)              0.4196   \n",
       "106             (humor)  (travel, language)              0.2862   \n",
       "107          (language)     (travel, humor)              0.3632   \n",
       "\n",
       "     consequent support  support  confidence      lift  leverage  conviction  \\\n",
       "0                0.4698   0.2226    0.503165  1.071019  0.014760    1.067154   \n",
       "1                0.4424   0.2226    0.473819  1.071019  0.014760    1.059711   \n",
       "2                0.2892   0.1496    0.338156  1.169279  0.021658    1.073968   \n",
       "3                0.4424   0.1496    0.517289  1.169279  0.021658    1.155143   \n",
       "4                0.4424   0.1865    0.407650  0.921452 -0.015898    0.941336   \n",
       "..                  ...      ...         ...       ...       ...         ...   \n",
       "103              0.2862   0.1457    0.623449  2.178368  0.078815    1.895626   \n",
       "104              0.4196   0.1457    0.717734  1.710520  0.060521    2.056216   \n",
       "105              0.2030   0.1457    0.347235  1.710520  0.060521    1.220961   \n",
       "106              0.2337   0.1457    0.509085  2.178368  0.078815    1.560961   \n",
       "107              0.1905   0.1457    0.401156  2.105808  0.076510    1.351772   \n",
       "\n",
       "     zhangs_metric  \n",
       "0         0.118919  \n",
       "1         0.125065  \n",
       "2         0.259634  \n",
       "3         0.203675  \n",
       "4        -0.135794  \n",
       "..             ...  \n",
       "103       0.705912  \n",
       "104       0.521182  \n",
       "105       0.715683  \n",
       "106       0.757832  \n",
       "107       0.824628  \n",
       "\n",
       "[108 rows x 10 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rules = association_rules(frequent_itemsets, metric=\"support\", min_threshold=0)\n",
    "\n",
    "rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support / Lift - Scatterplot"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AlbSchool",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
